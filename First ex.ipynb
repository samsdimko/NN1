{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow\n",
    "import numpy\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "numpy.random.seed(42)\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dmitry/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 800)               628000    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                8010      \n",
      "=================================================================\n",
      "Total params: 636,010\n",
      "Trainable params: 636,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitry/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(800, input_dim=784, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/dmitry/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(10, activation=\"softmax\", kernel_initializer=\"normal\")`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(800, input_dim = 784, init = \"normal\", activation = \"relu\"))\n",
    "model.add(Dense(10, init = \"normal\", activation = \"softmax\"))\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"SGD\", metrics = [\"accuracy\"])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitry/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dmitry/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/dmitry/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 15s 244us/step - loss: 1.2067 - acc: 0.7226\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.6028 - acc: 0.8584\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 0.4762 - acc: 0.8796\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 0.4182 - acc: 0.8909\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 0.3836 - acc: 0.8978\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.3596 - acc: 0.9030\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.3417 - acc: 0.9069\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.3273 - acc: 0.9106\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.3156 - acc: 0.9131\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.3054 - acc: 0.9156\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 12s 201us/step - loss: 0.2965 - acc: 0.9180\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 12s 201us/step - loss: 0.2887 - acc: 0.9201\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 12s 197us/step - loss: 0.2815 - acc: 0.9222\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 12s 201us/step - loss: 0.2748 - acc: 0.9243\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.2688 - acc: 0.9259\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.2632 - acc: 0.9270\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.2578 - acc: 0.9293\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.2530 - acc: 0.9305\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 11s 187us/step - loss: 0.2481 - acc: 0.9319\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.2436 - acc: 0.9331\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.2393 - acc: 0.9345\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 12s 196us/step - loss: 0.2351 - acc: 0.9357\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 17s 279us/step - loss: 0.2312 - acc: 0.9367\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.2275 - acc: 0.9377\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.2237 - acc: 0.9386\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.2202 - acc: 0.9396\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.2167 - acc: 0.9405\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.2134 - acc: 0.9414\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 0.2101 - acc: 0.9427\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.2071 - acc: 0.9433\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.2041 - acc: 0.9446\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 0.2011 - acc: 0.9451\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 0.1982 - acc: 0.9460\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.1954 - acc: 0.9467\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 12s 206us/step - loss: 0.1927 - acc: 0.9476\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.1901 - acc: 0.9483\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.1875 - acc: 0.9486\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.1850 - acc: 0.9493\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 13s 222us/step - loss: 0.1825 - acc: 0.9500\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 13s 222us/step - loss: 0.1801 - acc: 0.9506\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 13s 209us/step - loss: 0.1778 - acc: 0.9513\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 14s 232us/step - loss: 0.1756 - acc: 0.9519\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 14s 230us/step - loss: 0.1733 - acc: 0.9522\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 14s 234us/step - loss: 0.1712 - acc: 0.9530\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.1690 - acc: 0.9534\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 0.1669 - acc: 0.9538\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 12s 205us/step - loss: 0.1650 - acc: 0.9545\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.1630 - acc: 0.9548\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.1611 - acc: 0.9555\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 0.1592 - acc: 0.9563\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.1573 - acc: 0.9566\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.1556 - acc: 0.9573\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.1538 - acc: 0.9573\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.1521 - acc: 0.9580\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.1504 - acc: 0.9587\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.1488 - acc: 0.9590\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.1472 - acc: 0.9594\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.1456 - acc: 0.9600\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.1441 - acc: 0.9606\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.1425 - acc: 0.9608\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.1410 - acc: 0.9616\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.1396 - acc: 0.9621\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 0.1382 - acc: 0.9622\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 0.1368 - acc: 0.9628\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.1355 - acc: 0.9631\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.1341 - acc: 0.9636\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.1327 - acc: 0.9644\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.1314 - acc: 0.9642\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.1302 - acc: 0.9647\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.1290 - acc: 0.9649\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.1278 - acc: 0.9657\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: 0.1265 - acc: 0.9659\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.1253 - acc: 0.9661\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1241 - acc: 0.9663\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.1230 - acc: 0.9664\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1219 - acc: 0.9670\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.1208 - acc: 0.9672\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1197 - acc: 0.9677\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.1186 - acc: 0.9681\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.1177 - acc: 0.9682\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1166 - acc: 0.9683\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.1156 - acc: 0.9687\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1146 - acc: 0.9691\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1136 - acc: 0.9693\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1127 - acc: 0.9697\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.1117 - acc: 0.9698\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1108 - acc: 0.9701\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1098 - acc: 0.9704\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1089 - acc: 0.9704\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1081 - acc: 0.9708\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1072 - acc: 0.9711\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1063 - acc: 0.9715\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1055 - acc: 0.9716\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.1046 - acc: 0.9721\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.1038 - acc: 0.9724\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.1030 - acc: 0.9725\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 11s 187us/step - loss: 0.1022 - acc: 0.9726\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 12s 200us/step - loss: 0.1013 - acc: 0.9731\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.1006 - acc: 0.9732\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.0998 - acc: 0.9733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0f9539b810>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size = 200, nb_epoch = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 9\n",
    "plt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\n",
    "pred = model.predict(x_test[image_index].reshape(1, 784))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
